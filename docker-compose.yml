version: '3.8'

services:
  l9-graphrag:
    build:
      context: .
      dockerfile: docker/Dockerfile
    ports:
      - "43000:3000"
      - "49090:9090"  # Metrics
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=graphrag-password
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - LOG_LEVEL=INFO
      - ENABLE_METRICS=true
      - TOKENIZERS_PARALLELISM=false
      # Local cross-encoder reranker configuration
      - RERANKER_MODEL=BAAI/bge-reranker-base
      - RERANKER_MODEL_PATH=/app/models/reranker  # mount or bake model here
      - RERANK_BUDGET_MS=120
      - RERANK_CACHE_TTL=600
    volumes:
      - workspace:/workspace
      - ./config:/app/config
      - models:/app/models  # persist model weights/cache
    depends_on:
      neo4j:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  neo4j:
    image: neo4j:5.22-community
    ports:
      - "47474:7474"
      - "47687:7687"
    environment:
      - NEO4J_AUTH=neo4j/graphrag-password
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "graphrag-password", "RETURN 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  qdrant:
    image: qdrant/qdrant:v1.12.4
    ports:
      - "46333:6333"
      - "46334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    # Health check disabled - Qdrant container has minimal binaries
    # Service is functional and accessible on port 6333
    # healthcheck:
    #   test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:6333/collections || exit 1"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 20s

  # Redis for job queues - NO EVICTION, durable persistence
  redis-queue:
    image: redis:7-alpine
    ports:
      - "46380:6379"
    volumes:
      - redis_queue_data:/data
    command: >
      redis-server
      --appendonly yes
      --save 60 1
      --appendfsync everysec
      --no-appendfsync-on-rewrite yes
      --maxmemory-policy noeviction
      --requirepass ${REDIS_QUEUE_PASSWORD:-queue-secret-key}
    environment:
      - REDIS_PASSWORD=${REDIS_QUEUE_PASSWORD:-queue-secret-key}
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_QUEUE_PASSWORD:-queue-secret-key}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Redis for caching - EVICTION ENABLED, less critical persistence  
  redis-cache:
    image: redis:7-alpine
    ports:
      - "46379:6379"  # Keep existing port for cache
    volumes:
      - redis_cache_data:/data
    command: >
      redis-server
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --save 300 10
      --requirepass ${REDIS_CACHE_PASSWORD:-cache-secret-key}
    environment:
      - REDIS_PASSWORD=${REDIS_CACHE_PASSWORD:-cache-secret-key}
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_CACHE_PASSWORD:-cache-secret-key}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Neural Flow Embedding Service
  embeddings:
    image: neural-flow:nomic-v2-production
    container_name: neural-flow-nomic-v2-production
    ports:
      - "48000:8000"
    environment:
      - LOG_LEVEL=INFO
      - MODEL_NAME=nomic-embed-text-v1
      - BATCH_SIZE=32
      - MAX_TOKENS=8192
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - default

  # Gemma 2B Metadata Tagging Service
  gemma-tagger:
    image: ollama/ollama:latest
    container_name: neural-gemma-tagger
    volumes:
      - gemma_models:/root/.ollama
    ports:
      - "48001:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_NUM_PARALLEL=4
    networks:
      - default
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped

  # Neural Indexer Sidecar Service
  l9-indexer:
    build:
      context: .
      dockerfile: docker/Dockerfile.indexer
    container_name: l9-neural-indexer
    environment:
      # Service Configuration
      - PROJECT_PATH=/workspace
      - PROJECT_NAME=${PROJECT_NAME:-default}
      - INITIAL_INDEX=true
      - LOG_LEVEL=${LOG_LEVEL:-DEBUG}
      
      # Database Connections
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=graphrag-password
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      
      # Embedding Service (External Container)
      - EMBEDDING_SERVICE_HOST=embeddings
      - EMBEDDING_SERVICE_PORT=8000
      
      # Performance Tuning
      - BATCH_SIZE=10
      - DEBOUNCE_INTERVAL=2.0
      - MAX_QUEUE_SIZE=1000
      
      # Vector Configuration
      - EMBED_DIM=768
      
      # Feature Flags
      - STRUCTURE_EXTRACTION_ENABLED=true
      
      # Monitoring
      - ENABLE_METRICS=true
      - METRICS_PORT=8080
      
      # File watching optimization for Docker
      - WATCHDOG_FORCE_POLLING=1
      
    volumes:
      # Project Source (Read-Only) - override with PROJECT_HOST_DIR to mount any directory
      - ${PROJECT_HOST_DIR:-.}:/workspace:ro
      # Persistent State
      - indexer_state:/app/state
      # Configuration
      - ./neural-tools/config:/app/config:ro
      # Logs
      - indexer_logs:/app/logs
      
    ports:
      - "48080:8080"  # Health/Metrics endpoint
      
    depends_on:
      neo4j:
        condition: service_healthy
      qdrant:
        condition: service_started  # Changed from service_healthy since health check disabled
      redis-queue:
        condition: service_healthy
      redis-cache:
        condition: service_healthy
      embeddings:
        condition: service_healthy
        
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
      
    restart: unless-stopped
    
    # Resource Limits
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: "1.0"
        reservations:
          memory: 512M
          cpus: "0.5"
          
    # Security
    user: "1000:1000"
    security_opt:
      - no-new-privileges:true
    # NOTE: Removed read_only and tmpfs on /app/state & /app/logs to maintain persistence
    tmpfs:
      - /tmp:size=100M,noexec,nosuid,nodev
      
    networks:
      - default

  # Optional: Monitoring stack
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "49091:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "43001:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin

volumes:
  neo4j_data:
  neo4j_logs:
  neo4j_import:
  qdrant_data:
  redis_queue_data:  # Durable queue storage
  redis_cache_data:  # Cache storage
  workspace:
  prometheus_data:
  grafana_data:
  models:
  indexer_state:
    driver: local
  indexer_logs:
    driver: local
  gemma_models:  # Ollama model storage
    driver: local

networks:
  default:
    name: l9-graphrag-network
