version: '3.8'

services:
  l9-graphrag:
    build:
      context: .
      dockerfile: docker/Dockerfile
    ports:
      - "43000:3000"
      - "49090:9090"  # Metrics
    environment:
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USERNAME=neo4j
      - NEO4J_PASSWORD=graphrag-password
      - QDRANT_HOST=qdrant
      - QDRANT_PORT=6333
      - LOG_LEVEL=INFO
      - ENABLE_METRICS=true
      - TOKENIZERS_PARALLELISM=false
      # Local cross-encoder reranker configuration
      - RERANKER_MODEL=BAAI/bge-reranker-base
      - RERANKER_MODEL_PATH=/app/models/reranker  # mount or bake model here
      - RERANK_BUDGET_MS=120
      - RERANK_CACHE_TTL=600
    volumes:
      - workspace:/workspace
      - ./config:/app/config
      - models:/app/models  # persist model weights/cache
    depends_on:
      neo4j:
        condition: service_healthy
      qdrant:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  neo4j:
    image: neo4j:5.22-community
    ports:
      - "47474:7474"
      - "47687:7687"
    environment:
      - NEO4J_AUTH=neo4j/graphrag-password
      - NEO4J_PLUGINS=["apoc"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
      - neo4j_import:/var/lib/neo4j/import
    healthcheck:
      test: ["CMD", "cypher-shell", "-u", "neo4j", "-p", "graphrag-password", "RETURN 1"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  qdrant:
    image: qdrant/qdrant:latest  # v1.15.1+ for client compatibility
    ports:
      - "46333:6333"
      - "46334:6334"
    volumes:
      - qdrant_data:/qdrant/storage
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
    # Health check disabled - Qdrant container has minimal binaries
    # Service is functional and accessible on port 6333
    # healthcheck:
    #   test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:6333/collections || exit 1"]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 3
    #   start_period: 20s

  # Redis for job queues - NO EVICTION, durable persistence
  redis-queue:
    image: redis:7-alpine
    ports:
      - "46380:6379"
    volumes:
      - redis_queue_data:/data
    command: >
      redis-server
      --appendonly yes
      --save 60 1
      --appendfsync everysec
      --no-appendfsync-on-rewrite yes
      --maxmemory-policy noeviction
      --requirepass ${REDIS_QUEUE_PASSWORD:-queue-secret-key}
    environment:
      - REDIS_PASSWORD=${REDIS_QUEUE_PASSWORD:-queue-secret-key}
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_QUEUE_PASSWORD:-queue-secret-key}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Redis for caching - EVICTION ENABLED, less critical persistence  
  redis-cache:
    image: redis:7-alpine
    ports:
      - "46379:6379"  # Keep existing port for cache
    volumes:
      - redis_cache_data:/data
    command: >
      redis-server
      --maxmemory 512mb
      --maxmemory-policy allkeys-lru
      --save 300 10
      --requirepass ${REDIS_CACHE_PASSWORD:-cache-secret-key}
    environment:
      - REDIS_PASSWORD=${REDIS_CACHE_PASSWORD:-cache-secret-key}
    healthcheck:
      test: ["CMD", "redis-cli", "-a", "${REDIS_CACHE_PASSWORD:-cache-secret-key}", "ping"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s

  # Neural Flow Embedding Service
  embeddings:
    image: neural-flow:nomic-v2-production
    container_name: neural-flow-nomic-v2-production
    ports:
      - "48000:8000"
    environment:
      - LOG_LEVEL=INFO
      - MODEL_NAME=nomic-embed-text-v1
      - BATCH_SIZE=32
      - MAX_TOKENS=8192
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: unless-stopped
    networks:
      - default

  # Gemma 2B Metadata Tagging Service
  gemma-tagger:
    image: ollama/ollama:latest
    container_name: neural-gemma-tagger
    volumes:
      - gemma_models:/root/.ollama
    ports:
      - "48001:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_KEEP_ALIVE=5m
      - OLLAMA_NUM_PARALLEL=4
    networks:
      - default
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 120s
    restart: unless-stopped

  # ADR-0030: Indexer containers are now dynamically managed by IndexerOrchestrator
  # Each project gets its own indexer container spawned on-demand by MCP
  # The static l9-indexer service has been removed to eliminate parallel stacks
  # See docs/adr/0030-multi-container-indexer-orchestration.md for details

  # Optional: Monitoring stack
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "49091:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'

  grafana:
    image: grafana/grafana:latest
    ports:
      - "43001:3000"
    volumes:
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin

volumes:
  neo4j_data:
  neo4j_logs:
  neo4j_import:
  qdrant_data:
  redis_queue_data:  # Durable queue storage
  redis_cache_data:  # Cache storage
  workspace:
  prometheus_data:
  grafana_data:
  models:
  # ADR-0030: Removed indexer_state and indexer_logs volumes
  # Each dynamic indexer container manages its own state
  gemma_models:  # Ollama model storage
    driver: local

networks:
  default:
    name: l9-graphrag-network
