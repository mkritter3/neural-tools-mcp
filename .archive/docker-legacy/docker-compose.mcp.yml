# L9 Neural MCP + Qdrant - Production Configuration
# Complete MCP server running in Docker with hybrid search

version: '3.8'

services:
  # MCP Server with Neural Search
  mcp-server:
    build: 
      context: ..
      dockerfile: docker/Dockerfile.mcp
    image: l9-mcp-server:latest
    container_name: mcp-server-${PROJECT_NAME:-default}
    
    # STDIO transport for MCP communication
    stdin_open: true
    tty: true
    
    volumes:
      # Project source (read-only)
      - type: bind
        source: ${PROJECT_DIR:-../}
        target: /app/project
        read_only: true
      
      # Persistent data
      - type: bind
        source: ./.docker/mcp-data/${PROJECT_NAME:-default}
        target: /app/data
        bind:
          create_host_path: true
      
      # Model cache
      - type: volume
        source: mcp-models
        target: /app/models
    
    environment:
      # Project config
      - PROJECT_NAME=${PROJECT_NAME:-default}
      
      # Qdrant connection (internal Docker network)
      - QDRANT_HOST=qdrant
      - QDRANT_GRPC_PORT=6334
      
      # Embedding service connection
      - EMBEDDING_SERVICE_HOST=embedding-service
      - EMBEDDING_SERVICE_PORT=8000
      
      # Performance
      - OMP_NUM_THREADS=4
      - TOKENIZERS_PARALLELISM=false
      - PYTHONUNBUFFERED=1
    
    depends_on:
      qdrant:
        condition: service_healthy
      embedding-service:
        condition: service_healthy
    
    networks:
      - mcp-network
    
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '1.0'
    
    restart: unless-stopped
    
    labels:
      - "mcp.transport=stdio"
      - "mcp.project=${PROJECT_NAME:-default}"

  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:v1.10.0
    container_name: qdrant-${PROJECT_NAME:-default}
    
    # Only expose ports for debugging/admin (not for MCP)
    # MCP server communicates internally via Docker network
    ports:
      - "${QDRANT_DEBUG_PORT:-6680}:6333"  # REST API (debug only)
    
    volumes:
      # Persistent storage
      - type: bind
        source: ./.docker/qdrant/${PROJECT_NAME:-default}/storage
        target: /qdrant/storage
        bind:
          create_host_path: true
      
      - type: bind
        source: ./.docker/qdrant/${PROJECT_NAME:-default}/snapshots
        target: /qdrant/snapshots
        bind:
          create_host_path: true
    
    environment:
      # Qdrant configuration
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__SERVICE__HOST=0.0.0.0
      - QDRANT__LOG_LEVEL=INFO
      
      # Storage paths
      - QDRANT__STORAGE__STORAGE_PATH=/qdrant/storage
      - QDRANT__STORAGE__SNAPSHOTS_PATH=/qdrant/snapshots
      
      # Performance optimizations for hybrid search
      - QDRANT__STORAGE__PERFORMANCE__INDEXING_THRESHOLD_KB=20000
      - QDRANT__STORAGE__PERFORMANCE__MEMMAP_THRESHOLD_KB=50000
      - QDRANT__STORAGE__OPTIMIZERS__DEFAULT_SEGMENT_NUMBER=4
      
      # Enable both dense and sparse indexing
      - QDRANT__STORAGE__HNSW_INDEX__M=16
      - QDRANT__STORAGE__HNSW_INDEX__EF_CONSTRUCT=100
    
    networks:
      - mcp-network
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.5'
        reservations:
          memory: 1G
          cpus: '0.5'
    
    restart: unless-stopped

  # L9 Neural Flow - Existing Embedding Service
  embedding-service:
    image: neural-flow:l9-production
    container_name: neural-flow-${PROJECT_NAME:-default}
    
    command: ["python3", "-m", "uvicorn", "embedding_server:app", "--host", "0.0.0.0", "--port", "8000"]
    
    ports:
      - "${EMBEDDING_DEBUG_PORT:-8080}:8000"  # Debug access
    
    volumes:
      # Model cache (shared across projects) 
      - type: volume
        source: embedding-models
        target: /app/models
      
      # Bind mount for embedding server script
      - type: bind
        source: ./embedding_server.py
        target: /app/embedding_server.py
    
    environment:
      - EMBEDDING_MODEL=BAAI/bge-base-en-v1.5
      - MAX_CONCURRENT_REQUESTS=512
      - MAX_BATCH_TOKENS=16384
    
    networks:
      - mcp-network
    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 45s  # Time for model loading
    
    deploy:
      resources:
        limits:
          memory: 6G
          cpus: '3.0'
        reservations:
          memory: 3G
          cpus: '2.0'
    
    restart: unless-stopped
    
    labels:
      - "embedding.model=bge-base-en-v1.5"
      - "embedding.shared=true"
      - "service.type=neural-flow"

volumes:
  mcp-models:
    driver: local
  embedding-models:
    driver: local

networks:
  mcp-network:
    name: mcp-network-${PROJECT_NAME:-default}
    driver: bridge