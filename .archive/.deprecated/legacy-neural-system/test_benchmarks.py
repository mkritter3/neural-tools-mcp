#!/usr/bin/env python3
"""
Test script for Neural Flow Performance Benchmarking System
Validates benchmarking functionality with sample data
"""

import asyncio
import json
import tempfile
import logging
from pathlib import Path
import sys

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent))

from performance_benchmarks import PerformanceBenchmarks, WeeklyTargets
from neural_embeddings import CodeSpecificEmbedder
from neural_dynamic_memory_system import NeuralDynamicMemorySystem


async def test_benchmark_initialization():
    """Test benchmark system initialization"""
    print("üß™ Testing benchmark initialization...")
    
    with tempfile.TemporaryDirectory() as temp_dir:
        benchmarker = PerformanceBenchmarks(Path(temp_dir))
        
        # Test weekly target calculation
        targets = benchmarker.get_current_targets()
        assert isinstance(targets, WeeklyTargets)
        assert targets.recall_at_1 > 0
        assert targets.recall_at_3 > 0
        assert targets.latency_p95 > 0
        
        print(f"‚úÖ Current targets: Week {benchmarker.current_week}")
        print(f"   Recall@1: {targets.recall_at_1:.1%}")
        print(f"   Recall@3: {targets.recall_at_3:.1%}")
        print(f"   Latency P95: {targets.latency_p95}ms")
        
        return True


async def test_embedding_benchmarks():
    """Test embedding performance benchmarks"""
    print("\nüß™ Testing embedding benchmarks...")
    
    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            benchmarker = PerformanceBenchmarks(Path(temp_dir))
            
            # Use simple embedder for testing (fallback if Qodo not available)
            embedder = CodeSpecificEmbedder()
            
            # Sample code for testing
            test_code_samples = [
                "def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)",
                "async def fetch_data(): return await http.get('/api/data')",
                "class UserManager: def __init__(self): self.users = {}",
                "def validate_email(email): return '@' in email and '.' in email",
                "import json; data = json.loads(response.text)"
            ]
            
            # Run embedding benchmarks
            results = await benchmarker.benchmark_embedding_performance(
                embedder, test_code_samples
            )
            
            print(f"‚úÖ Embedding benchmarks completed:")
            for name, result in results.items():
                status = "‚úÖ PASS" if result.passed else "‚ùå FAIL"
                print(f"   {status} {result.metric}: {result.value:.3f} (target: {result.target:.3f})")
            
            return True
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Embedding benchmark test failed (expected in CI): {e}")
        return False


async def test_search_benchmarks():
    """Test search performance benchmarks"""
    print("\nüß™ Testing search benchmarks...")
    
    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            benchmarker = PerformanceBenchmarks(Path(temp_dir))
            
            # Initialize memory system with temp directory
            chroma_dir = Path(temp_dir) / "chroma_test"
            memory_system = NeuralDynamicMemorySystem(
                persist_directory=str(chroma_dir)
            )
            
            # Add some test data
            await memory_system.store_memory({
                "type": "code",
                "content": "def fibonacci(n): return n if n <= 1 else fibonacci(n-1) + fibonacci(n-2)",
                "metadata": {"function": "fibonacci", "language": "python"}
            })
            
            await memory_system.store_memory({
                "type": "code", 
                "content": "async def handle_request(req): return await process_request(req)",
                "metadata": {"function": "handle_request", "language": "python"}
            })
            
            # Test queries and ground truth
            test_queries = ["fibonacci calculation", "async request handler"]
            ground_truth = [["fibonacci", "recursive"], ["async", "request", "handler"]]
            
            # Run search benchmarks
            results = await benchmarker.benchmark_hybrid_search(
                memory_system, test_queries, ground_truth
            )
            
            print(f"‚úÖ Search benchmarks completed:")
            for name, result in results.items():
                status = "‚úÖ PASS" if result.passed else "‚ùå FAIL"
                print(f"   {status} {result.metric}: {result.value:.3f} (target: {result.target:.3f})")
            
            return True
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Search benchmark test failed: {e}")
        return False


async def test_full_benchmark_suite():
    """Test complete benchmark suite"""
    print("\nüß™ Testing full benchmark suite...")
    
    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            benchmarker = PerformanceBenchmarks(Path(temp_dir))
            
            # Run full benchmark suite (will likely fail without proper setup)
            results = await benchmarker.run_full_benchmark_suite()
            
            print(f"‚úÖ Full benchmark suite completed:")
            print(f"   Week: {results.get('week', 'unknown')}")
            
            if 'summary' in results:
                summary = results['summary']
                print(f"   Total tests: {summary.get('total_tests', 0)}")
                print(f"   Passed: {summary.get('passed_tests', 0)}")
                print(f"   Failed: {len(summary.get('failed_tests', []))}")
                
                if summary.get('failed_tests'):
                    print(f"   Failed tests: {', '.join(summary['failed_tests'])}")
            
            return True
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Full benchmark test failed (expected without models): {e}")
        return False


async def test_report_generation():
    """Test performance report generation"""
    print("\nüß™ Testing report generation...")
    
    try:
        with tempfile.TemporaryDirectory() as temp_dir:
            benchmarker = PerformanceBenchmarks(Path(temp_dir))
            
            # Generate report (even if no data exists)
            report = await benchmarker.generate_performance_report()
            
            assert isinstance(report, str)
            assert "Neural Flow Performance Report" in report
            assert f"Week {benchmarker.current_week}" in report
            
            print(f"‚úÖ Report generation successful:")
            print("   " + report[:200].replace('\n', '\n   ') + "...")
            
            return True
            
    except Exception as e:
        print(f"‚ùå Report generation test failed: {e}")
        return False


async def main():
    """Run all benchmark tests"""
    print("üöÄ Starting Neural Flow Benchmark Tests\n")
    
    # Set up logging
    logging.basicConfig(level=logging.WARNING)  # Reduce noise
    
    tests = [
        ("Initialization", test_benchmark_initialization),
        ("Embedding Benchmarks", test_embedding_benchmarks),
        ("Search Benchmarks", test_search_benchmarks),  
        ("Full Suite", test_full_benchmark_suite),
        ("Report Generation", test_report_generation),
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            success = await test_func()
            results.append((test_name, success))
        except Exception as e:
            print(f"‚ùå {test_name} test crashed: {e}")
            results.append((test_name, False))
    
    # Summary
    print(f"\nüìä Test Results Summary:")
    passed = sum(1 for _, success in results if success)
    total = len(results)
    
    for test_name, success in results:
        status = "‚úÖ PASS" if success else "‚ùå FAIL"
        print(f"   {status} {test_name}")
    
    print(f"\nüéØ Overall: {passed}/{total} tests passed")
    
    if passed == total:
        print("üéâ All benchmark tests passed!")
        return 0
    elif passed > 0:
        print("‚ö†Ô∏è  Some tests failed (may be expected in minimal environments)")
        return 0
    else:
        print("‚ùå All tests failed - check configuration")
        return 1


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    exit(exit_code)