#!/usr/bin/env python3
"""
ADR-66 + ADR-67: Unified Graphiti Service Client
Interfaces with containerized Graphiti service for temporal knowledge graphs
with unified Neo4j vector storage
"""

import os
import logging
import asyncio
import httpx
from typing import Dict, List, Any, Optional
from datetime import datetime
from pathlib import Path

logger = logging.getLogger(__name__)


class UnifiedGraphitiClient:
    """
    Client for containerized Graphiti service implementing ADR-66 + ADR-67
    Provides temporal knowledge graph capabilities with unified Neo4j storage
    """

    def __init__(self, project_name: str):
        self.project_name = project_name
        self.graphiti_host = os.getenv("GRAPHITI_HOST", "localhost")
        self.graphiti_port = os.getenv("GRAPHITI_PORT", "48080")
        self.base_url = f"http://{self.graphiti_host}:{self.graphiti_port}"

        # HTTP client with timeout configuration
        self.client = httpx.AsyncClient(
            timeout=httpx.Timeout(30.0),
            limits=httpx.Limits(max_connections=10, max_keepalive_connections=5)
        )

        logger.info(f"ðŸ”— UnifiedGraphitiClient initialized for project: {project_name}")
        logger.info(f"ðŸ“¡ Graphiti service: {self.base_url}")

    async def health_check(self) -> Dict[str, Any]:
        """Check Graphiti service health"""
        try:
            response = await self.client.get(f"{self.base_url}/health")
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Graphiti health check failed: {e}")
            return {"status": "error", "message": str(e)}

    async def add_code_episode(
        self,
        file_path: str,
        content: str,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Add code file as episodic knowledge to temporal graph
        ADR-67: Episodic processing for incremental, conflict-resistant indexing
        """
        try:
            episode_data = {
                "content": content,
                "episode_type": "text",
                "group_id": self.project_name,
                "metadata": {
                    "file_path": file_path,
                    "file_type": "code",
                    "timestamp": datetime.now().isoformat(),
                    "project": self.project_name,
                    **(metadata or {})
                }
            }

            response = await self.client.post(
                f"{self.base_url}/projects/{self.project_name}/episodes",
                json=episode_data
            )
            response.raise_for_status()

            result = response.json()
            logger.info(f"âœ… Code episode added for {file_path}: {result.get('episode_id')}")
            return result

        except Exception as e:
            logger.error(f"Failed to add code episode for {file_path}: {e}")
            return {"success": False, "error": str(e)}

    async def add_documentation_episode(
        self,
        file_path: str,
        content: str,
        doc_type: str = "markdown",
        metadata: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Add documentation as episodic knowledge with relationship detection
        ADR-69: LLM enhanced relationship detection between docs and code
        """
        try:
            episode_data = {
                "content": content,
                "episode_type": "text",
                "group_id": self.project_name,
                "metadata": {
                    "file_path": file_path,
                    "file_type": "documentation",
                    "doc_type": doc_type,
                    "timestamp": datetime.now().isoformat(),
                    "project": self.project_name,
                    **(metadata or {})
                }
            }

            response = await self.client.post(
                f"{self.base_url}/projects/{self.project_name}/episodes",
                json=episode_data
            )
            response.raise_for_status()

            result = response.json()
            logger.info(f"âœ… Documentation episode added for {file_path}: {result.get('episode_id')}")
            return result

        except Exception as e:
            logger.error(f"Failed to add documentation episode for {file_path}: {e}")
            return {"success": False, "error": str(e)}

    async def search_knowledge_graph(
        self,
        query: str,
        limit: int = 10,
        group_id: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Search temporal knowledge graph with hybrid capabilities
        ADR-66: Unified Neo4j storage with vector + graph + full-text search
        """
        try:
            search_data = {
                "query": query,
                "limit": limit,
                "group_id": group_id or self.project_name
            }

            response = await self.client.post(
                f"{self.base_url}/projects/{self.project_name}/search",
                json=search_data
            )
            response.raise_for_status()

            result = response.json()
            logger.info(f"ðŸ” Knowledge graph search completed: {len(result.get('results', []))} results")
            return result

        except Exception as e:
            logger.error(f"Knowledge graph search failed: {e}")
            return {"success": False, "error": str(e), "results": []}

    async def get_project_status(self) -> Dict[str, Any]:
        """Get project's knowledge graph status and statistics"""
        try:
            response = await self.client.get(f"{self.base_url}/projects")
            response.raise_for_status()

            projects_data = response.json()
            is_active = self.project_name in projects_data.get("projects", [])

            return {
                "project_name": self.project_name,
                "is_active": is_active,
                "total_projects": projects_data.get("total", 0)
            }

        except Exception as e:
            logger.error(f"Failed to get project status: {e}")
            return {"project_name": self.project_name, "is_active": False, "error": str(e)}

    async def cleanup_project(self) -> Dict[str, Any]:
        """Clean up project's temporal knowledge graph"""
        try:
            response = await self.client.delete(f"{self.base_url}/projects/{self.project_name}")
            response.raise_for_status()

            result = response.json()
            logger.info(f"ðŸ§¹ Project {self.project_name} knowledge graph cleaned up")
            return result

        except Exception as e:
            logger.error(f"Failed to cleanup project {self.project_name}: {e}")
            return {"success": False, "error": str(e)}

    async def close(self):
        """Close HTTP client"""
        await self.client.aclose()
        logger.info(f"ðŸ”Œ UnifiedGraphitiClient closed for project: {self.project_name}")


class UnifiedIndexerService:
    """
    ADR-66 + ADR-67: Enhanced indexer using unified Neo4j + Graphiti architecture
    Eliminates dual-write complexity while adding temporal knowledge capabilities
    """

    def __init__(self, project_name: str):
        self.project_name = project_name
        self.graphiti_client = UnifiedGraphitiClient(project_name)
        logger.info(f"ðŸš€ UnifiedIndexerService initialized for project: {project_name}")

    async def initialize(self) -> Dict[str, Any]:
        """Initialize unified indexer service"""
        try:
            # Check Graphiti service health
            health = await self.graphiti_client.health_check()

            if health.get("status") == "healthy":
                logger.info(f"âœ… Unified indexer ready for project: {self.project_name}")
                return {
                    "success": True,
                    "architecture": "unified_neo4j_graphiti",
                    "project": self.project_name,
                    "graphiti_health": health
                }
            else:
                logger.warning(f"âš ï¸ Graphiti service not healthy: {health}")
                return {
                    "success": False,
                    "error": "Graphiti service unhealthy",
                    "health": health
                }

        except Exception as e:
            logger.error(f"Failed to initialize unified indexer: {e}")
            return {"success": False, "error": str(e)}

    async def process_file(self, file_path: str) -> Dict[str, Any]:
        """
        Process file using unified Neo4j + Graphiti temporal knowledge architecture
        ADR-66: No dual-write complexity, ADR-67: Episodic processing
        """
        try:
            path_obj = Path(file_path)

            # Read file content
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            # Determine file type and processing strategy
            if path_obj.suffix in ['.py', '.js', '.ts', '.java', '.cpp', '.rs', '.go']:
                # Code file - add as code episode
                result = await self.graphiti_client.add_code_episode(
                    file_path=str(file_path),
                    content=content,
                    metadata={
                        "language": path_obj.suffix[1:],
                        "size": len(content),
                        "lines": len(content.split('\n'))
                    }
                )
            elif path_obj.suffix in ['.md', '.rst', '.txt', '.adoc']:
                # Documentation file - add as documentation episode
                result = await self.graphiti_client.add_documentation_episode(
                    file_path=str(file_path),
                    content=content,
                    doc_type=path_obj.suffix[1:],
                    metadata={
                        "size": len(content),
                        "lines": len(content.split('\n'))
                    }
                )
            else:
                logger.info(f"â­ï¸ Skipping unsupported file type: {file_path}")
                return {"success": True, "action": "skipped", "reason": "unsupported_type"}

            if result.get("success"):
                logger.info(f"âœ… File processed via unified architecture: {file_path}")
                return {
                    "success": True,
                    "architecture": "unified_neo4j_graphiti",
                    "episode_id": result.get("episode_id"),
                    "file_path": file_path
                }
            else:
                logger.error(f"âŒ Failed to process file: {file_path} - {result.get('error')}")
                return {
                    "success": False,
                    "error": result.get("error"),
                    "file_path": file_path
                }

        except Exception as e:
            logger.error(f"Exception processing file {file_path}: {e}")
            return {"success": False, "error": str(e), "file_path": file_path}

    async def search_unified_knowledge(self, query: str, limit: int = 10) -> Dict[str, Any]:
        """
        ADR-0066: Elite GraphRAG search using Neo4j vector indexes + hybrid capabilities
        Direct Neo4j access for optimal performance, bypassing HTTP overhead
        """
        try:
            # Initialize Neo4j service for direct vector search
            from servers.services.neo4j_service import Neo4jService
            from servers.services.nomic_local_service import NomicService

            neo4j_service = Neo4jService(self.project_name)
            nomic_service = NomicService()

            # Initialize services
            neo4j_init = await neo4j_service.initialize()
            nomic_init = await nomic_service.initialize()

            if not neo4j_init.get("success") or not nomic_init.get("success"):
                logger.warning("Falling back to Graphiti HTTP search due to service initialization failure")
                return await self.graphiti_client.search_knowledge_graph(query, limit)

            # Get embedding for the query using Nomic
            query_embeddings = await nomic_service.get_embeddings([query])
            if not query_embeddings or len(query_embeddings) == 0:
                logger.warning("Failed to get query embedding, falling back to text search")
                return {
                    "status": "partial_success",
                    "results": await neo4j_service.semantic_search(query, limit),
                    "search_type": "text_only",
                    "query": query
                }

            query_embedding = query_embeddings[0]

            # ADR-0066: Use Neo4j hybrid search for elite performance
            hybrid_results = await neo4j_service.hybrid_search(
                query_text=query,
                query_embedding=query_embedding,
                limit=limit,
                vector_weight=0.7  # Prioritize semantic similarity
            )

            # Format results for compatibility
            formatted_results = []
            for result in hybrid_results:
                node = result["node"]
                formatted_result = {
                    "content": node.get("content", node.get("text", "")),
                    "file_path": node.get("path", node.get("file_path", "")),
                    "chunk_id": node.get("chunk_id"),
                    "similarity_score": result["combined_score"],
                    "vector_score": result["vector_score"],
                    "text_score": result["text_score"],
                    "node_type": result["node_type"],
                    "search_method": "neo4j_hybrid_vector",
                    "metadata": {
                        "line_start": node.get("start_line"),
                        "line_end": node.get("end_line"),
                        "chunk_type": node.get("chunk_type"),
                        "language": node.get("language"),
                        "project": node.get("project")
                    }
                }
                formatted_results.append(formatted_result)

            logger.info(f"âœ… ADR-0066 elite search completed: {len(formatted_results)} results with Neo4j vector indexes")

            return {
                "status": "success",
                "results": formatted_results,
                "search_type": "neo4j_elite_hybrid",
                "query": query,
                "total_results": len(formatted_results),
                "performance_note": "Using HNSW vector indexes for O(log n) similarity search"
            }

        except Exception as e:
            logger.error(f"Elite GraphRAG search failed, falling back to Graphiti: {e}")
            # Fallback to original Graphiti search
            return await self.graphiti_client.search_knowledge_graph(query, limit)

    async def get_status(self) -> Dict[str, Any]:
        """Get unified indexer service status"""
        return await self.graphiti_client.get_project_status()

    async def cleanup(self) -> Dict[str, Any]:
        """Clean up unified indexer resources"""
        cleanup_result = await self.graphiti_client.cleanup_project()
        await self.graphiti_client.close()
        return cleanup_result