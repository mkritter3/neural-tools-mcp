name: Critical Regression Tests
# Validates critical bug fixes and architectural decisions

on:
  workflow_call:
    inputs:
      python-version:
        description: 'Python version'
        type: string
        default: '3.11'
    outputs:
      status:
        description: 'Regression test status'
        value: ${{ jobs.regression.outputs.status }}

env:
  NEO4J_PASSWORD: 'graphrag-password'

jobs:
  regression:
    name: Regression Tests
    runs-on: ubuntu-latest
    outputs:
      status: ${{ steps.result.outputs.status }}

    services:
      neo4j:
        image: neo4j:5.22.0
        env:
          NEO4J_AUTH: neo4j/${{ env.NEO4J_PASSWORD }}
          NEO4J_PLUGINS: '["apoc"]'
        ports:
          - 47687:7687
        options: >-
          --health-cmd "cypher-shell -u neo4j -p ${{ env.NEO4J_PASSWORD }} 'RETURN 1'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      qdrant:
        image: qdrant/qdrant:v1.12.5
        ports:
          - 46333:6333
        options: >-
          --health-cmd "curl -f http://localhost:6333/health || exit 1"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis-cache:
        image: redis:7-alpine
        ports:
          - 46379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      redis-queue:
        image: redis:7-alpine
        ports:
          - 46380:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - uses: actions/checkout@v5

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: ${{ inputs.python-version }}

      - name: Install Python dependencies
        working-directory: neural-tools
        run: |
          pip install -r config/requirements-indexer-lean.txt
          pip install pytest pytest-asyncio pytest-timeout docker

      - name: Wait for services
        run: |
          for i in {1..30}; do
            if curl -f http://localhost:46333/health && \
               nc -z localhost 47687 && \
               redis-cli -h localhost -p 46379 ping && \
               redis-cli -h localhost -p 46380 ping; then
              echo "All services are up!"
              break
            fi
            echo "Waiting for services... (attempt $i/30)"
            sleep 2
          done

      - name: Run ADR-0058 Indexer Regression Test
        id: adr058
        working-directory: .
        env:
          NEO4J_URI: bolt://localhost:47687
          NEO4J_PASSWORD: ${{ env.NEO4J_PASSWORD }}
          QDRANT_HOST: localhost
          QDRANT_PORT: 46333
          REDIS_CACHE_HOST: localhost
          REDIS_CACHE_PORT: 46379
          REDIS_QUEUE_HOST: localhost
          REDIS_QUEUE_PORT: 46380
          PYTHONPATH: ${{ github.workspace }}/neural-tools/src:${{ github.workspace }}
        run: |
          echo "🔍 Testing ADR-0058: Circular Dependency Fix"
          if python scripts/test-indexer-fix-adr0058.py; then
            echo "✅ ADR-0058 test passed"
            echo "adr058=success" >> $GITHUB_OUTPUT
          else
            echo "❌ ADR-0058 test failed - CRITICAL REGRESSION DETECTED"
            echo "adr058=failure" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Run Collection Naming Consistency Test (ADR-0057)
        id: adr057
        working-directory: neural-tools
        run: |
          echo "🔍 Testing ADR-0057: Collection Naming Consistency"
          if python tests/test_collection_naming_consistency.py; then
            echo "✅ ADR-0057 test passed"
            echo "adr057=success" >> $GITHUB_OUTPUT
          else
            echo "❌ ADR-0057 test failed - naming inconsistency detected"
            echo "adr057=failure" >> $GITHUB_OUTPUT
            # Non-blocking for now as there are known exceptions
          fi

      - name: Run Project Isolation Test (ADR-0029)
        id: adr029
        working-directory: neural-tools
        env:
          NEO4J_URI: bolt://localhost:47687
          NEO4J_PASSWORD: ${{ env.NEO4J_PASSWORD }}
        run: |
          echo "🔍 Testing ADR-0029: Project Isolation"
          # Create test script inline for ADR-0029 validation
          cat > test_adr029.py << 'EOF'
          import asyncio
          from neo4j import AsyncGraphDatabase
          import os

          async def test_project_isolation():
              uri = os.getenv('NEO4J_URI', 'bolt://localhost:47687')
              password = os.getenv('NEO4J_PASSWORD', 'graphrag-password')

              driver = AsyncGraphDatabase.driver(uri, auth=("neo4j", password))

              try:
                  async with driver.session() as session:
                      # Create test data for two projects
                      await session.run(
                          "CREATE (f1:File {path: '/test1.py', project: 'project-a'}) "
                          "CREATE (f2:File {path: '/test2.py', project: 'project-b'})"
                      )

                      # Query should only return data from one project
                      result = await session.run(
                          "MATCH (f:File {project: 'project-a'}) RETURN count(f) as count"
                      )
                      record = await result.single()

                      if record['count'] != 1:
                          print(f"❌ Expected 1 file for project-a, got {record['count']}")
                          return False

                      # Clean up
                      await session.run("MATCH (f:File) WHERE f.path IN ['/test1.py', '/test2.py'] DELETE f")

                      print("✅ Project isolation working correctly")
                      return True
              finally:
                  await driver.close()

          if __name__ == "__main__":
              success = asyncio.run(test_project_isolation())
              exit(0 if success else 1)
          EOF

          if python test_adr029.py; then
            echo "✅ ADR-0029 test passed"
            echo "adr029=success" >> $GITHUB_OUTPUT
          else
            echo "❌ ADR-0029 test failed - project isolation broken"
            echo "adr029=failure" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Run ADR-64 Mount Validation Tests (Unit Tests)
        id: adr064-unit
        working-directory: tests
        env:
          PYTHONPATH: ${{ github.workspace }}/neural-tools/src:${{ github.workspace }}
        run: |
          echo "🔍 Testing ADR-64: Mount Validation (Unit Tests)"
          if python test_indexer_mount_validation.py; then
            echo "✅ ADR-64 unit tests passed"
            echo "adr064_unit=success" >> $GITHUB_OUTPUT
          else
            echo "❌ ADR-64 unit tests failed - mount validation regression detected"
            echo "adr064_unit=failure" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: Run ADR-64 Mount Validation Tests (Integration Tests)
        id: adr064-integration
        working-directory: tests
        env:
          PYTHONPATH: ${{ github.workspace }}/neural-tools/src:${{ github.workspace }}
          NEO4J_URI: bolt://localhost:47687
          NEO4J_PASSWORD: ${{ env.NEO4J_PASSWORD }}
          QDRANT_HOST: localhost
          QDRANT_PORT: 46333
          REDIS_CACHE_HOST: localhost
          REDIS_CACHE_PORT: 46379
        run: |
          echo "🔍 Testing ADR-64: Mount Validation (Integration Tests)"
          if python integration/test_indexer_mount_validation.py; then
            echo "✅ ADR-64 integration tests passed"
            echo "adr064_integration=success" >> $GITHUB_OUTPUT
          else
            echo "❌ ADR-64 integration tests failed - Docker mount validation broken"
            echo "adr064_integration=failure" >> $GITHUB_OUTPUT
            # Mark as warning since Docker might not be available in CI
            echo "⚠️  Integration test failure - Docker may not be available"
          fi

      - name: Generate Test Report
        if: always()
        run: |
          echo "## 📊 Regression Test Report"
          echo ""
          echo "| ADR | Test | Status |"
          echo "|-----|------|--------|"
          echo "| ADR-0058 | Circular Dependency Fix | ${{ steps.adr058.outputs.adr058 == 'success' && '✅ PASS' || '❌ FAIL' }} |"
          echo "| ADR-0057 | Collection Naming | ${{ steps.adr057.outputs.adr057 == 'success' && '✅ PASS' || '⚠️ WARN' }} |"
          echo "| ADR-0029 | Project Isolation | ${{ steps.adr029.outputs.adr029 == 'success' && '✅ PASS' || '❌ FAIL' }} |"
          echo "| ADR-0064 | Mount Validation (Unit) | ${{ steps.adr064-unit.outputs.adr064_unit == 'success' && '✅ PASS' || '❌ FAIL' }} |"
          echo "| ADR-0064 | Mount Validation (Integration) | ${{ steps.adr064-integration.outputs.adr064_integration == 'success' && '✅ PASS' || '⚠️ WARN' }} |"

      - name: Set output status
        id: result
        if: always()
        run: |
          if [[ "${{ steps.adr058.outputs.adr058 }}" == "failure" ]] || \
             [[ "${{ steps.adr029.outputs.adr029 }}" == "failure" ]] || \
             [[ "${{ steps.adr064-unit.outputs.adr064_unit }}" == "failure" ]]; then
            echo "status=failure" >> $GITHUB_OUTPUT
            echo "❌ Critical regression tests failed"
            exit 1
          else
            echo "status=success" >> $GITHUB_OUTPUT
            echo "✅ All critical regression tests passed"
            # Note: ADR-064 integration tests are warning-only since Docker may not be available in CI
          fi