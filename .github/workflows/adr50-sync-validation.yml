name: ADR-0050 Neo4j-Qdrant Sync Validation

on:
  push:
    paths:
      - 'neural-tools/src/servers/services/indexer_service.py'
      - 'neural-tools/src/servers/services/multi_project_indexer.py'
      - 'docker/scripts/indexer-entrypoint.py'
      - 'neural-tools/tests/test_adr50_sync_validation.py'
      - 'neural-tools/tests/test_neo4j_qdrant_sync.py'
  pull_request:
    paths:
      - 'neural-tools/src/servers/services/**'
  schedule:
    # Run daily at 2 AM UTC to catch drift
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      debug_enabled:
        type: boolean
        description: 'Enable debug logging'
        required: false
        default: false

jobs:
  sync-validation:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    services:
      neo4j:
        image: neo4j:5.22.0
        env:
          NEO4J_AUTH: neo4j/graphrag-password
          NEO4J_PLUGINS: '["apoc"]'
          NEO4J_apoc_export_file_enabled: true
          NEO4J_apoc_import_file_enabled: true
        ports:
          - 47687:7687
        options: >-
          --health-cmd "cypher-shell -u neo4j -p graphrag-password 'RETURN 1'"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

      qdrant:
        image: qdrant/qdrant:v1.15.1
        ports:
          - 46333:6333
        options: >-
          --health-cmd "curl -f http://localhost:6333/health"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r neural-tools/requirements.txt
          pip install pytest pytest-asyncio pytest-cov pytest-timeout

      - name: Wait for services
        run: |
          echo "Waiting for Neo4j..."
          timeout 60 bash -c 'until nc -z localhost 47687; do sleep 1; done'
          echo "Waiting for Qdrant..."
          timeout 60 bash -c 'until nc -z localhost 46333; do sleep 1; done'
          echo "Services ready!"

      - name: Initialize test data
        run: |
          # Create test project in databases
          python3 << EOF
          import asyncio
          from neo4j import GraphDatabase
          from qdrant_client import QdrantClient
          from qdrant_client.models import VectorParams, Distance

          async def init_test_data():
              # Initialize Neo4j
              driver = GraphDatabase.driver("bolt://localhost:47687", auth=("neo4j", "graphrag-password"))
              with driver.session() as session:
                  # Create constraints
                  session.run("""
                      CREATE CONSTRAINT IF NOT EXISTS FOR (f:File)
                      REQUIRE (f.project, f.path) IS UNIQUE
                  """)
                  session.run("""
                      CREATE CONSTRAINT IF NOT EXISTS FOR (c:Chunk)
                      REQUIRE (c.project, c.chunk_id) IS UNIQUE
                  """)

                  # Create test file
                  session.run("""
                      MERGE (f:File {project: 'test-project', path: '/test/file.py'})
                      SET f.indexed_at = datetime()
                  """)

              # Initialize Qdrant
              client = QdrantClient(host='localhost', port=46333)

              # Create collection
              client.create_collection(
                  collection_name="project-test-project",
                  vectors_config=VectorParams(size=768, distance=Distance.COSINE)
              )

              print("Test data initialized")

          asyncio.run(init_test_data())
          EOF

      - name: Run ADR-0050 validation tests
        env:
          NEO4J_URI: bolt://localhost:47687
          NEO4J_PASSWORD: graphrag-password
          QDRANT_HOST: localhost
          QDRANT_PORT: 46333
          PROJECT_NAME: test-project
          DEBUG: ${{ github.event.inputs.debug_enabled }}
        run: |
          cd neural-tools
          python -m pytest tests/test_adr50_sync_validation.py -v \
            --tb=short \
            --timeout=60 \
            --cov=src/servers/services \
            --cov-report=xml \
            --cov-report=term-missing \
            --junitxml=test-results.xml

      - name: Run original sync tests
        if: always()
        run: |
          cd neural-tools
          python -m pytest tests/test_neo4j_qdrant_sync.py -v \
            --tb=short \
            --timeout=60

      - name: Upload test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: test-results
          path: |
            neural-tools/test-results.xml
            neural-tools/coverage.xml

      - name: Generate compliance report
        if: always()
        run: |
          python3 << EOF
          import json
          import xml.etree.ElementTree as ET
          from datetime import datetime

          # Parse test results
          try:
              tree = ET.parse('neural-tools/test-results.xml')
              root = tree.getroot()

              tests = int(root.attrib.get('tests', 0))
              failures = int(root.attrib.get('failures', 0))
              errors = int(root.attrib.get('errors', 0))
              passed = tests - failures - errors

              print("# ADR-0050 Sync Validation Report")
              print(f"**Date**: {datetime.now().isoformat()}")
              print(f"**Status**: {'✅ PASS' if failures == 0 and errors == 0 else '❌ FAIL'}")
              print(f"**Tests**: {passed}/{tests} passed")
              print("")

              if failures > 0 or errors > 0:
                  print("## Failed Tests")
                  for testcase in root.findall('.//testcase'):
                      failure = testcase.find('failure')
                      if failure is not None:
                          print(f"- {testcase.attrib.get('name')}")
                          print(f"  ```{failure.attrib.get('message', 'Unknown failure')}```")

              print("")
              print("## Recommendations")

              if passed == tests:
                  print("✅ All sync validation tests passing")
                  print("✅ System meets ADR-0050 requirements")
              else:
                  print("⚠️ Sync validation failures detected")
                  print("⚠️ Review failed tests and implement fixes")
                  print("⚠️ Ensure Chunk node creation is implemented in indexer")

              print("")
              print("## Next Steps for September 2025")
              print("1. Implement CDC with Debezium for transaction log streaming")
              print("2. Add self-healing reconciliation service")
              print("3. Deploy OpenTelemetry distributed tracing")

          except FileNotFoundError:
              print("⚠️ No test results found - tests may have failed to run")
          EOF > sync-report.md

      - name: Comment PR with results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('sync-report.md', 'utf8');

            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: report
            });

      - name: Check sync rate threshold
        if: always()
        run: |
          # This would connect to real services and check sync rate
          # For CI, we're validating the test framework exists
          python3 << EOF
          print("Checking sync rate thresholds...")
          # In production, this would query actual sync metrics
          # and fail the build if sync rate < 95%
          EOF

  integration-test:
    needs: sync-validation
    runs-on: ubuntu-latest
    if: success()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Build indexer image
        run: |
          docker build -t l9-neural-indexer:ci-test \
            -f docker/neural-indexer.dockerfile .

      - name: Test container startup
        run: |
          docker run -d \
            --name test-indexer \
            -e PROJECT_NAME=test-project \
            -e PROJECT_PATH=/workspace \
            -e NEO4J_URI=bolt://host.docker.internal:47687 \
            -e NEO4J_PASSWORD=graphrag-password \
            -e QDRANT_HOST=host.docker.internal \
            -e QDRANT_PORT=46333 \
            l9-neural-indexer:ci-test

          # Wait for health endpoint
          sleep 10

          # Check health
          docker exec test-indexer curl -f http://localhost:8080/health || exit 1

          # Check status
          docker exec test-indexer curl -f http://localhost:8080/status || exit 1

          # Cleanup
          docker stop test-indexer
          docker rm test-indexer

      - name: Validate production readiness
        run: |
          echo "✅ ADR-0050 validation complete"
          echo "✅ Sync mechanisms tested"
          echo "✅ Container integration verified"